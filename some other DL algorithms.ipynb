{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff47c49-a18b-4a7b-ade2-ec9e03aab508",
   "metadata": {},
   "source": [
    "<img src=\"./fig/NNandDL.png\" width=\"80%\">  \n",
    "\n",
    "### <font color=green>**循环神经网络**  \n",
    "带有反向传播-记忆功能的神经网络  \n",
    "\n",
    "#### <font color=green>**Why RNN?**  \n",
    "- **前馈网络的问题**  \n",
    "  对于CNN这种前馈网络，相邻两层之间存在单向连接，层内无连接→有向无环图  \n",
    "  输入和输出维数固定，神经元数量固定→无法处理变长的序列数据  \n",
    "  每次网络的输出只依赖于当前输入→没有记忆  \n",
    "- **可计算问题**  \n",
    "  函数-前馈网络  \n",
    "  有限状态机&图灵机  需要记忆能力  \n",
    "\n",
    "#### <font color=green>**有哪些能够实现记忆的模型？**  \n",
    "- **时延神经网络TDNN**  \n",
    "  建立额外延时单元，用以储存网络的历史信息（输入、输出、隐状态）\n",
    "  简单来说就是给函数的各个变量加一个时间下标  \n",
    "  短期记忆能力  \n",
    "  <img src=\"./fig/TDNN.png\" width=\"30%\">  \n",
    "- **自回归模型AR**  \n",
    "  一类时间序列模型 用自己过去的值$y_{t-i}$ 预测 现在的值$y_{t}$\n",
    "  \n",
    "- **有外部输入的非线性自回归模型NARX**  \n",
    "  <img src=\"./fig/NARX.png\" width=\"40%\">  \n",
    "- **循环神经网络**  \n",
    "\n",
    "#### **<font color=green>循环神经网络RNN**  \n",
    "利用带自反馈的神经元，处理任意长度的时序数据  \n",
    "<img src=\"./fig/RNN.png\" width=\"40%\">  \n",
    "一种比前馈神经网络更符合生物神经网络的结构  \n",
    "广泛运用于语音识别、语言模型、自然语言等和时序相关的生成任务上  \n",
    "将循环神经网络按时间展开  \n",
    "<img src=\"./fig/timeexpand.png\" width=\"40%\">  \n",
    "时间维度上很深，非时间维上很浅  \n",
    "\n",
    "- **简单循环神经网络SRN**  \n",
    "  状态更新的方式： $h_{t}=f(Uh_{t-1}+Wx_{t}+b)$  \n",
    "  Theorem:一个完全连接的循环神经网络是任何非线性动力系统的近似器  \n",
    "  图灵完备性：完全连接的循环神经网络可近似解决所有可计算问题  \n",
    "- **RNN的作用**  \n",
    "    - 输入输出映射-机器学习模型  \n",
    "    - 存储器-联想记忆模型Hopfield  \n",
    "\n",
    "- **将循环神经网络应用到机器学习**  \n",
    "    - 序列到类别（指输入与输出）  \n",
    "    <img src=\"./fig/seqcat.png\" width=\"40%\">  \n",
    "    $h_{T}$包含所有的序列信息：有点太长了→改为按时间平均采样  \n",
    "    e.g 对文本序列记性情感分类  \n",
    "    汉字转向量→一串汉字作为一个时间序列  \n",
    "    - 同步的序列到序列  \n",
    "    <img src=\"./fig/seqseq.png\" width=\"40%\">  \n",
    "     输入与输出具有一一对应关系  \n",
    "     e.g.1 中文分词问题 把一句句子分成词语 S-单字词 B-词语开始 E-词语结束  \n",
    "     e.g.2 信息抽取 从无结构文本中抽取结构化信息形成知识  \n",
    "     e.g.3 语音识别 CTC模型  \n",
    "    - 异步的序列到序列模式  \n",
    "    即错位的对应关系，还有自回归性  \n",
    "    <img src=\"./fig/wrongseqseq.png\" width=\"40%\">  \n",
    "    e.g 机器翻译 每步生成一个词  \n",
    "\n",
    "- **参数学习**  \n",
    "定义损失  \n",
    "<img src=\"./fig/indexlearn.png\" width=\"40%\">  \n",
    "计算损失函数的梯度  \n",
    "<img src=\"./fig/RNNgrad.png\" width=\"40%\">  \n",
    "U在所有$h$的连接上，W在所有$x$到$h$的连接上  \n",
    "<img src=\"./fig/gradcal.png\" width=\"20%\">  \n",
    "**随时间的反向传播算法BPTT**  \n",
    "做一个近似：每个点的导数不依赖于时间，就会发现$t-k \\to \\infty$时发生梯度消失或梯度爆炸  \n",
    "这等效于一个截断效果！→**长程依赖问题**  \n",
    "由于梯度爆炸或消失，实际上只能学习短周期的依赖关系  \n",
    "\n",
    "- **如何解决长程依赖问题**  \n",
    "    - $\\gamma=1$？  \n",
    "      精心挑选参数以在初始化时使它为1  \n",
    "      循环边改为线性依赖关系$h_{t}=h_{t-1}+g(Wx_{t}+b)$  \n",
    "      但是这样模型能力会变弱，所以进一步在$g$上增加一些关于$h_{t-1}$的非线性项（有点类似残差网络）  \n",
    "    - 对梯度爆炸-权重衰减或权重截断  \n",
    "    - 对梯度消失问题-优化模型  \n",
    "\n",
    "- **常用的循环神经网络模型**  \n",
    "为解决长程依赖问题引入门控机制-控制信息的累积速度，有选择地加入新的信息，有选择的遗忘之前累积的信息  \n",
    "基于门控的循环神经网络 Gated RNN  \n",
    "    - 门控循环单元 GRU  \n",
    "    - 长短期记忆网络LST  \n",
    "- **GRU**  \n",
    "用更新门$z_{t}$控制线性与非线性信息的比例    \n",
    "<img src=\"./fig/renew.png\" width=\"40%\">  \n",
    "用重置门$r_{t}$进一步控制序列信息和输入信息的比例  \n",
    "<img src=\"./fig/reset.png\" width=\"40%\">  \n",
    "整个GRU的结构  \n",
    "<img src=\"./fig/GRU.png\" width=\"40%\">  \n",
    "\n",
    "- **长短期记忆神经网络LSTM**  \n",
    "<img src=\"./fig/LSTM.png\" width=\"40%\">  \n",
    "引入新的内部记忆单元$c$作为线性项，更好地释放$h$训练非线性  \n",
    "$c_{t}$由输入门$i_{t}$和遗忘门$f_{t}$组成  \n",
    "LSTM的一些变体  \n",
    "<img src=\"./fig/LSTMchange.png\" width=\"40%\">  \n",
    "\n",
    "- **深层循环神经网络**  \n",
    "增加非时间维度上的深度  \n",
    "    - **堆叠循环神经网络**  \n",
    "      <img src=\"./fig/stackRNN.png\" width=\"40%\">  \n",
    "      井字形展开模型\n",
    "      或者一些变体：某一层可以接受来自下一层的所有时刻的信息/某一个时刻可以接受该时刻所有层的信息  \n",
    "    - **双向循环神经网络**  \n",
    "      <img src=\"./fig/mutualRNN.png\" width=\"40%\">  \n",
    "      既可以按照顺时序建模，也可以按照逆时序建模→可以获得来自两个方向的信息  \n",
    "\n",
    "- **循环神经网络应用**  \n",
    "    - 自然语言理解  \n",
    "      一个句子的可能性/合理性 通过打分判断  \n",
    "      如何打分？  \n",
    "      <img src=\"./fig/nlanguage.png\" width=\"40%\">  \n",
    "      <img src=\"./fig/languagemodel.png\" width=\"40%\">  \n",
    "    - 机器翻译  \n",
    "      传统方式  \n",
    "      <img src=\"./fig/translate.png\" width=\"40%\">  \n",
    "      RNN方式：很自然地得到词语排列顺序  \n",
    "    - 看图说话  \n",
    "      CNN+RNN  \n",
    "      <img src=\"./fig/imagecontent.png\" width=\"40%\">  \n",
    "    - 写字  \n",
    "      把每一个字母看做一连串的点，一个字母的写法是每一个点相对于前一个点的偏移量；再增加一维判断是否要提笔  \n",
    "    - 问答模型  \n",
    "      <img src=\"./fig/communication.png\" width=\"40%\">  \n",
    "      把问题/对话 作为可被记忆的历史序列  \n",
    "\n",
    "- **扩展到图结构**\n",
    "序列（循环神经网络）→树（递归神经网络）→图（图网络）  \n",
    "    - 树结构  \n",
    "      程序语言/自然语言的句法结构  \n",
    "      递归神经网络在一个有向无循环（树）上共享一个组合函数  \n",
    "      <img src=\"./fig/tree.png\" width=\"40%\">  \n",
    "      递归神经网络可以退化为循环神经网络  \n",
    "    - 图网络  \n",
    "      <img src=\"./fig/imgnet.png\" width=\"40%\">  \n",
    "      定义图、节点与连接  \n",
    "      更新一张图：根据连接节点+全局更新边；根据边+全局再更新节点，最后根据新的图更新全局信息  \n",
    "      更新函数&读出函数  \n",
    "\n",
    "### **<font color=green>注意力机制与外部记忆**  \n",
    "这两个是部件级别的模块-可以与其他的网络相融合，增强网络的能力    \n",
    "#### **<font color=green>注意力机制**  \n",
    "如何在大量信息中筛选出相关的信息  \n",
    "- **注意力机制**  \n",
    "  why要引入注意力机制？ e.g 阅读理解  \n",
    "  类似于大脑，每时每刻接收的信息非常多；如何解决信息超载?→注意力集中  \n",
    "    - 自下而上  \n",
    "      自动地被突出的信息吸引--汇聚pooling  \n",
    "    - 自上而下\n",
    "      主动根据任务将注意力集中到某一方面--汇聚focus  \n",
    "      我们期望在机器学习中实现的应该是这样一种注意力机制  \n",
    "      从输入向量中筛选出和q相关的  \n",
    "      <img src=\"./fig/focus.png\" width=\"30%\">  \n",
    "      hard attention - 只找到最相关的→离散无梯度，要用强化学习    \n",
    "      soft attention - 按相关度对所有信息进行加权汇总  \n",
    "      <img src=\"./fig/softattention.png\" width=\"40%\">  \n",
    "      有哪些注意力打分函数  \n",
    "      <img src=\"./fig/attentionevaluate.png\" width=\"30%\">  \n",
    "      变体：键值对注意力 key-value pair attention  \n",
    "      <img src=\"./fig/pairattention.png\" width=\"40%\">  \n",
    "      变体：多头注意力 multi-head attention  \n",
    "      用多个查询同时从输入信息中读取多组信息；每个注意力头关注输入信息的不同部分  \n",
    "    - 指针网络 \n",
    "      只利用注意力机制中的第一步，将注意力分布作为一个软性指针指出相关信息  \n",
    "      比如在阅读理解中指出答案所在的位置  \n",
    "\n",
    "- **注意力机制的应用**  \n",
    "    - 文本分类问题  \n",
    "      可以根据不同的标准 e.g,情感分类 对象分类  使用不同的筛选向量$q$  \n",
    "      <img src=\"./fig/textsort.png\" width=\"40%\">  \n",
    "    - 机器翻译的优化  \n",
    "      传统RNN一般采用序列-序列的机器翻译模式  但序列重建事实上是比较复杂的任务  \n",
    "      采用注意力机制后 可以在每次获取下一个词时集中注意力到上一个词的上下文结构 即$h_{t-1}=q_{t}$  \n",
    "      <img src=\"./fig/atttranslate.png\" width=\"40%\">  \n",
    "      <img src=\"./fig/attentionshow.png\" width=\"30%\"> 注意力分布示意图  \n",
    "    - Image Caption  \n",
    "      每次集中注意力于图像的不同部位以生成对应的词语  \n",
    "      <img src=\"./fig/attimgcap.png\" width=\"40%\">  \n",
    "    - 阅读理解  \n",
    "      一种双向的注意力机制 从问题筛选原文+从原文筛选问题  \n",
    "      <img src=\"./fig/readcom.png\" width=\"30%\">  \n",
    "\n",
    "- **变长序列建模--自注意力模型**  \n",
    "处理变长序列时，可以使用RNN/CNN来得到一个相同长度的输出序列  But这样的到的依赖关系是local的  \n",
    "<img src=\"./fig/changelen.png\" width=\"40%\">  \n",
    "→建立非局部的依赖关系  全连接→自注意力模型  \n",
    "每个输出会与所有的输入信息有关，但所占的权重由神经网络本身的动态计算决定  \n",
    "可以想象动态的网络连接方式，而查询$q$是由当前的输入信息自身决定的  \n",
    "<img src=\"./fig/selfatten.png\" width=\"40%\">  \n",
    "用向量点乘与求和的方法计算权重  \n",
    "自注意力模型的矩阵表示  \n",
    "<img src=\"./fig/selfattenmatrix.png\" width=\"40%\">  \n",
    "    - 对自注意力模型的优化  \n",
    "      QKV模式  \n",
    "      对原向量用不同的矩阵进行处理后 分开query-key-value三者  \n",
    "      <img src=\"./fig/QKV.png\" width=\"40%\">  \n",
    "      <img src=\"./fig/QKVcal.png\" width=\"40%\">  \n",
    "      进一步优化还可以采用多头QKV模式  \n",
    "      同时对输入序列的多个特征进行计算  \n",
    "      <img src=\"./fig/multiQKV.png\" width=\"40%\">  \n",
    "\n",
    "- **Transformer**  \n",
    "  一套完整的自注意力网络只有自注意力机制是不够的的  \n",
    "  <img src=\"./fig/selfattennet.png\" width=\"40%\">  \n",
    "  其他操作：位置编码（包括进位置信息）、层归一化（为了能有更深的网络）、直连边、逐位的FFN（类似于做一个卷积操作） \n",
    "  相当于每个点都动态地与其他所有点相连  \n",
    "  <img src=\"./fig/selfattenill.png\" width=\"40%\">  \n",
    "  复杂度分析  \n",
    "  <img src=\"./fig/complexdegree.png\" width=\"40%\">  \n",
    "  整个transformer的encode和decode架构  \n",
    "  decode的特殊指出 mask attention（只能看到前面不能看到后面）+cross attention \n",
    "  需要经过大量数据预训练，避免在小数据集上的过拟合  \n",
    "\n",
    "- **外部记忆**  \n",
    "人的记忆：外部信息在人脑中的显示  \n",
    "记忆过程：短期记忆→情感记忆→结构/长期记忆  \n",
    "特点：联想记忆 只能靠相似的记忆触发 而不能像计算机一样直接寻址  \n",
    "<img src=\"./fig/memory.png\" width=\"40%\">  \n",
    "通常人脑中的记忆容量很少 →如何增加记忆容量？ 外部记忆网络  \n",
    "主网络与外部记忆单元可以记性读写操作  \n",
    "<img src=\"./fig/MNN.png\" width=\"40%\">  \n",
    "外部记忆单元如何设置？①结构化②联想记忆  \n",
    "    - 结构化外部记忆  \n",
    "      典型的是矩阵  \n",
    "      如何读写？  \n",
    "      在主网络中定义查询向量去外部记忆单元中查找（soft）  \n",
    "      比如阅读理解中 把整个story作为外部记忆（只读）  \n",
    "      <img src=\"./fig/outsidememory.png\" width=\"40%\">  \n",
    "      可读写的：神经图灵机  \n",
    "      用注意力机制产生一个寻址 在memory中读与写  \n",
    "      \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d2aea-4c3f-4249-af9f-52e74b3c2b24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
