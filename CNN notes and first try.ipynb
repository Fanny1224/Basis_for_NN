{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57296a7-24f5-41f8-b81f-3db47ca70afd",
   "metadata": {},
   "source": [
    "# 一些关于CNN（卷积神经网络）的基本知识\n",
    "\n",
    "### 概述\n",
    "\n",
    "深度学习算法在计算机视觉中有重要的应用  \n",
    "CNN是其中非常重要的一种：图像检测、分类和检索（识图）、超分辨率重构、人脸识别  \n",
    "核心在于**特征提取**  \n",
    "卷积神经网络与传统神经网络的区别： **高维化** 3D数据 h w c  \n",
    "整体架构：**输入层（数据集）+卷积层（特征提取）+池化层（压缩）+全连接层（输入层→隐层）**\n",
    "\n",
    "### *卷积层  \n",
    "做每张图片中不同的区域都有不同的重要性（e.g 主体、背景），因此要先对其进行区域划分，每个区域有不同的像素点  \n",
    "如何描述该区域的特征→计算**特征值**  \n",
    "特征值的得到通过一组权重参数→每个像素点按照不同的权重参数加权求和 即得到每个小区域的特征值 （按照求内积的规则）（有时还要加**常数bias修正**）\n",
    "权重参数和区域划分的选取：**卷积核** 是一种全局的filter （**卷积参数共享** why？减少参数个数，防止过拟合）\n",
    "\n",
    "为什么会有3D？  \n",
    "图像的**颜色通道**，即 c dimension  \n",
    "每个颜色通道要分别用相同的卷积核做计算，并对结果求和  \n",
    "而每次卷积完后输出结果的c是由卷积核的选取个数决定的\n",
    "\n",
    "不同的卷积核将给出不同的**特征图**→特征更加丰富  \n",
    "一共做多少次过滤才足够？对应着提取出不同层次的信息  \n",
    "而每一次过滤都在上一次得到的3D特征图的基础上，而不是在原始数据上  \n",
    "总体的参数包括：卷积核尺寸、滑动窗口步长、边缘填充、卷积核个数  \n",
    "不同的卷积核大小和滑动窗口步长将得到不同的特征图大小 步长越小图越大、越细节  \n",
    "一般来说对图像任务：步长1；文本识别任务：步长可以更大etc\n",
    "\n",
    "**边缘填充**：注意一个问题，边界上的点因为卷积核移动的限制，对特征值的贡献次数将更少  \n",
    "but 边界点不一定就是图像中不重要的点  \n",
    "那，怎么办？  \n",
    "和我们在沙堆中做的类似的 手动加上一圈数值为0的边界 （+pad 1）\n",
    "\n",
    "### *池化层\n",
    "因为卷积完后得到的矩阵非常大，所以要通过池化层进一步**压缩特征**\\\n",
    "池化算法有很多，几个栗子：\\\n",
    "**max pooling**:选出每个区域中最大的数，即提取最重要的特征 这是目前最普遍也最准确的做法\\\n",
    "**average pooling**\n",
    "\n",
    "### *感受野\n",
    "最后一步中一个特征值是由最初多少个格点参与计算得到的  \n",
    "感受野越大越好：一个特征值所囊括的特征越多  \n",
    "e.g why three 3乘3 instead of 7乘7 ? 更细致、更少参数、更多次线性操作\n",
    "\n",
    "\n",
    "### *整个网络的神经任务的进行\n",
    "  \n",
    "<img src=\"./fig/cnn (2).png\" width=\"40%\"/>\n",
    "\n",
    "每一次卷积之后都要加一层**非线性变换RELU**  \n",
    "在这些组合之间间隔地加入池化压缩层  \n",
    "最后的**全连接层FC**能给出图像对不同类别的符合程度（训练结果）  \n",
    "但FC只能处理一维数据，所以将卷积、池化后的3D矩阵拉成向量  \n",
    "\n",
    "层数计算：只有带参数计算的为一层 卷积层&FC层  \n",
    "\n",
    "#### 几种经典网络架构\n",
    "**Alexnet经典网络（2012）**    \n",
    "8层网络、5层卷积+3层全连接  \n",
    "问题在于卷积核太大（特征很粗）、没有加pad  \n",
    "<img src=\"./fig/alexnet.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "**VGG网络（2014）**  \n",
    "不同的版本 最主流的版本为红框内的  \n",
    "3*3卷积核 更细腻 网络层数更多  \n",
    "每次pooling之后，为了弥补损失的信息→特征图翻倍\n",
    "但训练时间更长（several days\n",
    "\n",
    "那是不是层数越多效果越好？the depth of learning？ That's not the case.  \n",
    "在之前的特征图基础上再提取不一定会有更好的果效  \n",
    "<img src=\"./fig/vgg.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "**残差网络Resnet**  \n",
    "筛选出多余的层中效果好的  \n",
    "同等映射：通过改变不同效果的层的权重来决定其是否取多大的作用  \n",
    "比较中间那部是否存在的效果来决定其权重  \n",
    "主要用于特征提取（网络有分类和回归的不同功）  \n",
    "\n",
    "<img src=\"./fig/resnet.png\" width=\"40%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371c48-6d16-49c5-984f-db303c3677ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd999182-5d34-4a1d-b0e8-6330cd8f26c9",
   "metadata": {},
   "source": [
    "# 下面是CNN的编程实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3085ace-b1d4-45aa-ac33-5dd3d10340bd",
   "metadata": {},
   "source": [
    "第一步，导入各个需要的库\n",
    "torch装了两百年..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0dac87-4b13-4d53-bf90-ba14f85e832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                    #torch是一个张量处理库\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms    #torchvision库中有相关的网络结构\n",
    "%matplotlib inline         \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b45c4-d7b2-4ddf-bd3f-ae7ff097343c",
   "metadata": {},
   "source": [
    "首先读取数据\n",
    "- 构建训练集和测试集\n",
    "- DataLoader来迭代取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72527836-bc31-46e8-8321-e9fcff481aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m \u001b[38;5;66;03m#一个批次64张图片\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#这里使用的都是python模块的内置数据集\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#训练集导入\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#测试集导入\u001b[39;00m\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\mnist.py:87\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset not found.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     91\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m You can use download=True to download it\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\mnist.py:176\u001b[0m, in \u001b[0;36mMNIST.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url))\n\u001b[1;32m--> 176\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download (trying next):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error)\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\utils.py:413\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    411\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 413\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(archive, extract_root))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\utils.py:139\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[1;32m--> 139\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\utils.py:33\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: response\u001b[38;5;241m.\u001b[39mread(chunk_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     35\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torchvision\\datasets\\utils.py:33\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     35\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:455\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 455\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:499\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    494\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#定义超参数\n",
    "input_size=28 #图像是28*28*1\n",
    "num_classes=10 #标签的种类数（指把图像分成多少类）\n",
    "num_epochs=3 #训练总循环周期\n",
    "batch_size=64 #一个批次64张图片\n",
    "\n",
    "#这里使用的都是python模块的内置数据集\n",
    "#训练集导入\n",
    "train_dataset=datasets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "\n",
    "#测试集导入\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#构建batch数据\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc4766-f1e3-46c9-88b6-49d1658a9934",
   "metadata": {},
   "source": [
    "卷积网络模块构建\n",
    "- 一般卷积层，relu层，池化层可以写成一个套餐\n",
    "- 注意卷积最后结果还是一个特征图，需要把图转换成向量才能做分类或者回归任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c896e-d762-483b-a83f-a9b864e57bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #第一个卷积套餐\n",
    "        self.conv1 = nn.Sequential(         # 输入大小 (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # 灰度图，如果RGB图就是3\n",
    "                out_channels=16,            # 要得到几多少个特征图\n",
    "                kernel_size=5,              # 卷积核大小 5*5\n",
    "                stride=1,                   # 步长\n",
    "                padding=2,                  # 即+pad几 如果希望卷积后大小跟原来一样，需要设置padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # 输出的特征图为 (16, 28, 28)\n",
    "            nn.ReLU(),                      # relu层\n",
    "            nn.MaxPool2d(kernel_size=2),    # 进行池化操作（2x2 区域）, 输出结果为： (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # 下一个套餐的输入 (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # 按照conv1中的参数顺序设置，输出 (32, 14, 14)\n",
    "            nn.ReLU(),                      # relu层\n",
    "            nn.MaxPool2d(2),                # 输出 (32, 7, 7)\n",
    "        )\n",
    "        \n",
    "        #最后一步是将三维数据拉成向量，再连上全连接层\n",
    "        #第一个参数为向量长度，第二个参数为图像种类数\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # 全连接层得到的结果\n",
    "\n",
    "   #连接网络，进行运算\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten操作，结果为：(batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776c2fa-805b-404e-b778-3cd49037d135",
   "metadata": {},
   "source": [
    "定义评估标准：准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ef91f-9b36-47cf-b0aa-ff480734f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred = torch.max(predictions.data, 1)[1] \n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum() \n",
    "    return rights, len(labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e979191-15b3-4e4c-912b-e9c059677cc8",
   "metadata": {},
   "source": [
    "接着就可以开始训练模型了:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694b3e0-0261-4ae3-8425-5bc511e40336",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 实例化\n",
    "net = CNN() \n",
    "#损失函数\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#优化器\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #定义优化器，普通的随机梯度下降算法\n",
    "\n",
    "#开始训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    #当前epoch的结果保存下来\n",
    "    train_rights = [] \n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  #针对容器中的每一个批进行循环\n",
    "        net.train()                             \n",
    "        output = net(data) \n",
    "        loss = criterion(output, target) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        right = accuracy(output, target) \n",
    "        train_rights.append(right) \n",
    "\n",
    "    \n",
    "        if batch_idx % 100 == 0: #每隔100次计算一下准确率\n",
    "            \n",
    "            net.eval() \n",
    "            val_rights = [] \n",
    "            \n",
    "            for (data, target) in test_loader:\n",
    "                output = net(data) \n",
    "                right = accuracy(output, target) \n",
    "                val_rights.append(right)\n",
    "                \n",
    "            #准确率计算\n",
    "            train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))\n",
    "            val_r = (sum([tup[0] for tup in val_rights]), sum([tup[1] for tup in val_rights]))\n",
    "\n",
    "            print('当前epoch: {} [{}/{} ({:.0f}%)]\\t损失: {:.6f}\\t训练集准确率: {:.2f}%\\t测试集正确率: {:.2f}%'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.data, \n",
    "                100. * train_r[0].numpy() / train_r[1], \n",
    "                100. * val_r[0].numpy() / val_r[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
